import os
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from typing import Dict, Any, List
from .base import BaseAgent
import json
from app.utils.logger import logger

load_dotenv()

class QuestionAgent(BaseAgent):
    def __init__(self):
        super().__init__("question_agent")
        self.llm = ChatAnthropic(model="claude-3-5-sonnet-20240620", anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.prompts = {
            'initial': "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ íƒœë¸”ë¦¿ì„ ì°¾ê³  ê³„ì‹ ì§€ íŽ¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš” ðŸ˜Š",
            'analyze_response': """ë‹¹ì‹ ì€ ì‚¬ìš©ìžì˜ íƒœë¸”ë¦¿ êµ¬ë§¤ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ëŠ” AI ì‹œìŠ¤í…œìž…ë‹ˆë‹¤.
            ì•„ëž˜ ì‚¬ìš©ìžì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ì •í™•ížˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
            ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
            
            ì‚¬ìš©ìž ì‘ë‹µ: {user_input}
            
            {{
                "included_info": {{
                    "budget": {{
                        "included": false,
                        "value": ""
                    }},
                    "preferred_brand": {{
                        "included": false,
                        "value": ""
                    }},
                    "purpose": {{
                        "included": false,
                        "value": ""
                    }}
                }},
                "missing_info": ["budget", "preferred_brand", "purpose"]
            }}""",
            'follow_up': {
                "budget": "ðŸ’° ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ë¡œ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”?",
                "preferred_brand": "âœ¨ íŠ¹ë³„ížˆ ì„ í˜¸í•˜ì‹œëŠ” ë¸Œëžœë“œê°€ ìžˆìœ¼ì‹ ê°€ìš”?",
                "purpose": "ðŸ“± íƒœë¸”ë¦¿ìœ¼ë¡œ ì£¼ë¡œ ì–´ë–¤ ê±¸ í•˜ì‹¤ ê³„íšì´ì‹ ê°€ìš”?"
            }
            ,
            'ask_additional': """âœ¨ ê¸°ë³¸ì ì¸ ë‚´ìš©ì€ ìž˜ ì•Œê² ìŠµë‹ˆë‹¤!
                í˜¹ì‹œ ì¶”ê°€ë¡œ ì›í•˜ì‹œëŠ” ê¸°ëŠ¥ì´ë‚˜ ê³ ë ¤ì‚¬í•­ì´ ìžˆìœ¼ì‹ ê°€ìš”?
                (ì˜ˆ: ì‚¬ìš© í™˜ê²½, ë§Œì¡±ë„ ì¤‘ìš” í•­ëª© ë“±)

                ðŸ’¡ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ìžì„¸í•œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:
                â€¢ ì£¼ë¡œ ì–´ë””ì„œ ì‚¬ìš©í•˜ì‹¤ ê³„íšì¸ê°€ìš”? (ì˜ˆ: ì¹´íŽ˜, ì´ë™ ì¤‘, ì‹¤ë‚´ ë“±)
                â€¢ í•˜ë£¨ í‰ê·  ì‚¬ìš© ì‹œê°„ì€ ì–´ëŠ ì •ë„ë¡œ ì˜ˆìƒí•˜ì‹œë‚˜ìš”?
                â€¢ íŠ¹ë³„ížˆ ì‹ ê²½ ì“°ì‹œëŠ” ë¶€ë¶„ì´ ìžˆë‚˜ìš”? (ì˜ˆ: ë°œì—´, ë°°í„°ë¦¬ ë“±)
                â€¢ ê±±ì •ë˜ëŠ” ë¶€ë¶„ì´ ìžˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”? (AS, ë‚´êµ¬ì„± ë“±)

                ì—†ìœ¼ì‹œë‹¤ë©´ 'ì—†ìŒ'ì´ë¼ê³  ë‹µë³€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤ ðŸ˜Š"""
                ,
            'requirements': """
                ë‹¹ì‹ ì€ íƒœë¸”ë¦¿ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ íƒœë¸”ë¦¿ êµ¬ë§¤ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
                
                ì‚¬ìš©ìž ì„¤ëª…: {user_input}
                
                ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
                
                1. í•µì‹¬ ìš”êµ¬ì‚¬í•­ (ê° í•­ëª© 50ìž ì´ë‚´):
                - ì‚¬ìš©ìžê°€ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•œ í•„ìˆ˜ ê¸°ëŠ¥ê³¼ ì„ í˜¸ì‚¬í•­
                - ì‚¬ìš© ëª©ì ê³¼ ê´€ë ¨ëœ ìž ìž¬ì  ìš”êµ¬ì‚¬í•­
                - ì‚¬ìš©ìžê°€ í‘œí˜„í•œ ìš°ë ¤ì‚¬í•­ì´ë‚˜ ê¸°ëŒ€

                2. ì˜ˆì‚° ë²”ìœ„ (ê° í•­ëª© 30ìž ì´ë‚´):
                - ëª…ì‹œëœ ì˜ˆì‚° ì •ë³´
                - ì˜ˆì‚° ê´€ë ¨ ìš°ë ¤ì‚¬í•­ì´ë‚˜ ê¸°ëŒ€
                
                3. ê¸°ìˆ  ìŠ¤íŽ™ (ê° í•­ëª© 50ìž ì´ë‚´):
                - ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
                - ì‚¬ìš© ëª©ì ì„ ê³ ë ¤í–ˆì„ ë•Œ í•„ìš”í•œ ìµœì†Œ ì‚¬ì–‘
                - í˜¸í™˜ì„±ì´ë‚˜ í™•ìž¥ì„± ê´€ë ¨ ìš”êµ¬ì‚¬í•­
                
            """,
            'confirmation': """ì§€ê¸ˆê¹Œì§€ ë§ì”€í•´ ì£¼ì‹  ë‚´ìš©ì„ ì •ë¦¬í•´ë³´ì•˜ì–´ìš”:

{requirements}

ðŸ’¡ ìš”êµ¬ì‚¬í•­ì´ ë§žìœ¼ì‹œë‹¤ë©´
"ok", "ë„¤", "ì¢‹ì•„ìš”", "ë§žì•„ìš”" ì¤‘ì— íŽ¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

âœï¸ ìˆ˜ì •ì´ í•„ìš”í•˜ì‹œë‹¤ë©´
ì–´ë–¤ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ë©´ ì¢‹ì„ì§€ ì•Œë ¤ì£¼ì„¸ìš”.
ë‹¤ì‹œ ì •ë¦¬í•´ë“œë¦´ê²Œìš”!""",
            'update_requirements': """
                ê¸°ì¡´ ìš”êµ¬ì‚¬í•­: {previous_requirements}
                ì‚¬ìš©ìž í”¼ë“œë°±: {user_feedback}
                
                ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
                1. ì œí’ˆ ì¹´í…Œê³ ë¦¬:
                2. í•µì‹¬ ìš”êµ¬ì‚¬í•­:
                3. ê¸°ìˆ  ìŠ¤íŽ™:
                4. ì˜ˆì‚° ë²”ìœ„:
            """
            }
        logger.debug("QuestionAgent initialized")

    def _is_confirmation_response(self, response: str) -> bool:
        positive_responses = ['ok', 'yes', 'ì‘', 'ë„¤', 'ë§žì•„', 'ì¢‹ìŠµë‹ˆë‹¤', 'ê´œì°®ìŠµë‹ˆë‹¤']
        return response.lower().strip() in positive_responses

    async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
        try:
            if "conversation_history" not in state:
                return {
                    "response": self.prompts['initial'],
                    "conversation_history": [],
                    "requirements": None,
                    "collected_info": {},
                    "status": "collecting_initial"
                }
            
            user_input = state.get("user_input", "").strip()
            conversation_history = state.get("conversation_history", [])
            current_status = state.get("status", "collecting_initial")
            collected_info = state.get("collected_info", {})

            if current_status == "collecting_initial":
                analysis_prompt = self.prompts['analyze_response'].format(user_input=user_input)
                analysis_response = await self.llm.ainvoke(analysis_prompt)
                content = analysis_response.content if hasattr(analysis_response, 'content') else str(analysis_response)
                
                try:
                    content = content.strip()
                    content = content.replace('```json', '').replace('```', '').strip()
                    analysis = json.loads(content)
                    
                    # ìˆ˜ì§‘ëœ ì •ë³´ ì €ìž¥
                    for key, info in analysis['included_info'].items():
                        if info['included']:
                            collected_info[key] = info['value']

                    # ëˆ„ë½ëœ ì •ë³´ê°€ ìžˆëŠ” ê²½ìš°
                    missing_info = analysis.get('missing_info', [])
                    if missing_info:
                        next_question = self.prompts['follow_up'][missing_info[0]]
                        return {
                            "response": next_question,
                            "conversation_history": conversation_history + [
                                {"role": "user", "content": user_input},
                                {"role": "assistant", "content": next_question}
                            ],
                            "collected_info": collected_info,
                            "missing_info": missing_info,
                            "current_question": missing_info[0],
                            "status": "collecting_missing"
                        }
                    else:
                        return await self._proceed_to_requirements(collected_info, conversation_history, user_input)

                except json.JSONDecodeError as e:
                    logger.error(f"JSON parsing error: {e}")
                    return self._handle_error("ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

            elif current_status == "collecting_missing":
                # í˜„ìž¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì €ìž¥
                current_question = state.get("current_question")
                missing_info = state.get("missing_info", [])
                
                if current_question:
                    collected_info[current_question] = user_input
                    # í˜„ìž¬ ì§ˆë¬¸ì„ missing_infoì—ì„œ ì œê±°
                    missing_info = [info for info in missing_info if info != current_question]

                # ë‚¨ì€ ì§ˆë¬¸ì´ ìžˆëŠ”ì§€ í™•ì¸
                if missing_info:
                    next_question = self.prompts['follow_up'][missing_info[0]]
                    return {
                        "response": next_question,
                        "conversation_history": conversation_history + [
                            {"role": "user", "content": user_input},
                            {"role": "assistant", "content": next_question}
                        ],
                        "collected_info": collected_info,
                        "missing_info": missing_info,
                        "current_question": missing_info[0],
                        "status": "collecting_missing"
                    }
                else:
                    # ëª¨ë“  í•„ìˆ˜ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ë©´ ì¶”ê°€ ìš”êµ¬ì‚¬í•­ í™•ì¸
                    return {
                        "response": self.prompts['ask_additional'],
                        "conversation_history": conversation_history + [
                            {"role": "user", "content": user_input},
                            {"role": "assistant", "content": self.prompts['ask_additional']}
                        ],
                        "collected_info": collected_info,
                        "status": "asking_additional"
                    }

            elif current_status == "asking_additional":
                if user_input.lower().strip() != "ì—†ìŒ":
                    # ì¶”ê°€ ìš”êµ¬ì‚¬í•­ì„ collected_infoì— ì €ìž¥
                    collected_info["additional_requirements"] = user_input
                
                # ìš”êµ¬ì‚¬í•­ ë¶„ì„ìœ¼ë¡œ ì§„í–‰
                return await self._proceed_to_requirements(collected_info, conversation_history, user_input)

            elif current_status == "confirming_requirements":
                # ì‚¬ìš©ìž í™•ì¸ ì‘ë‹µ ì²´í¬
                if self._is_confirmation_response(user_input):
                    # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
                    return {
                        "response": state.get("requirements"),
                        "conversation_history": conversation_history + [
                            {"role": "user", "content": user_input}
                        ],
                        "requirements": state.get("requirements"),
                        "status": "requirements_collected"
                    }
                else:
                    # ìˆ˜ì • ìš”ì²­ ì²˜ë¦¬
                    previous_requirements = state.get("requirements", "")
                    prompt = self.prompts['update_requirements'].format(
                        previous_requirements=previous_requirements,
                        user_feedback=user_input
                    )
                    updated_requirements_response = await self.llm.ainvoke(prompt)
                    updated_requirements = updated_requirements_response.content if hasattr(updated_requirements_response, 'content') else str(updated_requirements_response)
                    
                    return {
                        "response": self.prompts['confirmation'].format(requirements=updated_requirements),
                        "conversation_history": conversation_history + [
                            {"role": "user", "content": user_input},
                            {"role": "assistant", "content": self.prompts['confirmation'].format(requirements=updated_requirements)}
                        ],
                        "requirements": updated_requirements,
                        "status": "confirming_requirements"
                    }
        
        except Exception as e:
            logger.error(f"Error in run: {e}")
            return self._handle_error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        
    def _handle_error(self, message: str) -> Dict[str, Any]:
        return {
            "response": message,
            "status": "error",
            "conversation_history": [],
            "requirements": None
        }

    async def _prepare_agent_states(self, requirements: str) -> Dict[str, Any]:
        try:
            spec_agent_state = await self._prepare_spec_agent_state(requirements)
            review_agent_state = await self._prepare_review_agent_state(requirements)
            youtube_agent_state = await self._prepare_youtube_agent_state(requirements)

            return {
                "youtube_agent_state": youtube_agent_state,
                "review_agent_state": review_agent_state,
                "spec_agent_state": spec_agent_state
            }
        except Exception as e:
            logger.error(f"Error in _prepare_agent_states: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ìƒíƒœ ë°˜í™˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ê°€ ê³„ì† ì§„í–‰ë  ìˆ˜ ìžˆë„ë¡ í•¨
            return {
                "youtube_agent_state": {"youtube_analysis": {}},
                "review_agent_state": {"review_analysis": {}},
                "spec_agent_state": {"spec_analysis": {}}
            }

    async def _prepare_spec_agent_state(self, requirements: str) -> Dict[str, Any]:
        try:
            prompt = f"""
            ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ ì œí’ˆì˜ ê¸°ìˆ  ìŠ¤íŽ™ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
            
            {requirements}
            
            ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”):
            {{
                "í•„ìˆ˜_ìŠ¤íŽ™": {{
                    "ì„±ëŠ¥": ["í•„ìˆ˜ì ì¸ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ë“¤"],
                    "í•˜ë“œì›¨ì–´": ["í•„ìˆ˜ì ì¸ í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ë“¤"],
                    "ê¸°ëŠ¥": ["í•„ìˆ˜ì ì¸ ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ë“¤"]
                }},
                "ì„ í˜¸_ìŠ¤íŽ™": {{
                    "ì„±ëŠ¥": ["ì„ í˜¸í•˜ëŠ” ì„±ëŠ¥ íŠ¹ì„±ë“¤"],
                    "í•˜ë“œì›¨ì–´": ["ì„ í˜¸í•˜ëŠ” í•˜ë“œì›¨ì–´ íŠ¹ì„±ë“¤"],
                    "ê¸°ëŠ¥": ["ì„ í˜¸í•˜ëŠ” ê¸°ëŠ¥ë“¤"]
                }},
                "ì œì™¸_ìŠ¤íŽ™": ["í”¼í•´ì•¼ í•  íŠ¹ì„±ë“¤"],
                "ê°€ê²©_ë²”ìœ„": {{
                    "ìµœì†Œ": {{"value": ìµœì†Œ ê°€ê²©, "unit": "KRW"}},
                    "ìµœëŒ€": {{"value": ìµœëŒ€ ê°€ê²©, "unit": "KRW"}}
                }}
            }}
            """

            response = await self.llm.ainvoke(prompt)
            content = response.content if hasattr(response, 'content') else str(response)

            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
            logger.debug(f"LLM Response content: {content}")

            # JSON ë¬¸ìžì—´ ì •ì œ
            content = content.strip()
            if content.startswith("```json"):
                content = content.split("```json")[1]
            if content.startswith("```"):
                content = content.split("```")[1]
            content = content.strip()

            parsed_content = json.loads(content)
            return {"spec_analysis": parsed_content}
        except Exception as e:
            logger.error(f"Error in _prepare_spec_agent_state: {e}")
            logger.error(f"Failed content: {content}")
            raise

    async def _prepare_review_agent_state(self, requirements: str) -> Dict[str, Any]:
        content = None  # content ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜
        try:
            prompt = f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒœë¸”ë¦¿ ì‚¬ìš©ìžì˜ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ê³ , ë¦¬ë·° ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ë¶„ì„ ì‹œ ë‹¤ìŒ ì›ì¹™ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì‚¬ìš©ìžê°€ ì§ì ‘ ì–¸ê¸‰í•œ ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ í¬í•¨
2. ê²€ìƒ‰ ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ìƒëžµ
4. ê° í•­ëª©ì€ ìµœëŒ€ 3ë‹¨ì–´ ì´ë‚´ì˜ ê°„ê²°í•œ í‚¤ì›Œë“œë¡œ í‘œí˜„

ê° ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¹¨:

[ì£¼ìš”_í™œë™]
- íƒœë¸”ë¦¿ìœ¼ë¡œ ì‹¤ì œ ìˆ˜í–‰í•  êµ¬ì²´ì ì¸ ìž‘ì—…ì´ë‚˜ ìš©ë„ë¥¼ í¬í•¨
- ì˜ˆì‹œ: "ì˜ìƒ íŽ¸ì§‘", "ë””ì§€í„¸ ë“œë¡œìž‰", "ë¬¸ì„œ ìž‘ì—…", "ê²Œìž„", "ì˜í™” ì‹œì²­"
- ì¶”ìƒì  í‘œí˜„("ì—”í„°í…Œì¸ë¨¼íŠ¸") ëŒ€ì‹  êµ¬ì²´ì  í™œë™("ë„·í”Œë¦­ìŠ¤ ì‹œì²­")ìœ¼ë¡œ í‘œí˜„

[ì‚¬ìš©_í™˜ê²½]
- íƒœë¸”ë¦¿ì„ ì£¼ë¡œ ì‚¬ìš©í•  ë¬¼ë¦¬ì  ìž¥ì†Œë‚˜ ìƒí™©
- ì˜ˆì‹œ: "ì‹¤ë‚´", "ì¹´íŽ˜", "ì´ë™ ì¤‘", "ì¹¨ëŒ€ì—ì„œ", "ì•¼ì™¸"

[ì‚¬ìš©_ì‹œê°„]
- ì¼ì¼ ë˜ëŠ” ì£¼ê°„ ì˜ˆìƒ ì‚¬ìš© ì‹œê°„
- ì˜ˆì‹œ: "í•˜ë£¨ 2ì‹œê°„", "5ì‹œê°„ ì´ìƒ", "ê°„í—ì  ì‚¬ìš©"

[ì‚¬ìš©ìž_ìˆ˜ì¤€]
- ì‚¬ìš©ìžì˜ ê¸°ìˆ ì  ìˆ™ë ¨ë„ë‚˜ ì „ë¬¸ì„±
- ì˜ˆì‹œ: "ì´ˆë³´ìž", "ì „ë¬¸ê°€", "ì·¨ë¯¸ ìž‘ê°€", "í•™ìƒ"

[ë¸Œëžœë“œ_ì„ í˜¸ë„]
- ì„ í˜¸í•˜ëŠ” íŠ¹ì • ì œì¡°ì‚¬ë‚˜ ë¸Œëžœë“œ ì´ë¦„
- ì˜ˆì‹œ: "ì• í”Œ", "ì‚¼ì„±", "ë ˆë…¸ë²„"

[ë¶ˆíŽ¸ì‚¬í•­]
- ê¸°ì¡´ ì œí’ˆ ì‚¬ìš©ì‹œ ê²½í—˜í–ˆê±°ë‚˜ ìš°ë ¤ë˜ëŠ” ë¬¸ì œì 
- ì˜ˆì‹œ: "ë°œì—´", "ë°°í„°ë¦¬", "ë¬´ê±°ì›€", "ì§€ì—°ì‹œê°„"

[ë§Œì¡±ë„_ì¤‘ìš”í•­ëª©]
- ì‚¬ìš©ìžê°€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê¸°ëŠ¥ì´ë‚˜ íŠ¹ì„±
- ì˜ˆì‹œ: "íŽœ ì§€ì›", "í™”ë©´ í¬ê¸°", "í”„ë¡œì„¸ì„œ ì„±ëŠ¥", "ë² ì ¤ ë‘ê»˜"

[ë””ìžì¸_ì„ í˜¸ë„]
- ë¯¸ì  ì„ í˜¸ë„ë‚˜ ë¬¼ë¦¬ì  ë””ìžì¸ ìš”ì†Œ
- ì˜ˆì‹œ: "ì–‡ì€ ë””ìžì¸", "ë©”íƒˆ ìž¬ì§ˆ", "ê°€ë²¼ìš´ ë¬´ê²Œ"

[ê°€ê²©ëŒ€_ì‹¬ë¦¬]
- ê°€ê²©ì— ëŒ€í•œ íƒœë„ë‚˜ ì˜ˆì‚° ë²”ìœ„
- ì˜ˆì‹œ: "ê°€ì„±ë¹„", "í”„ë¦¬ë¯¸ì—„", "ìƒê´€ì—†ìŒ"

[ì‚¬ìš©ìž_ìš°ë ¤ì‚¬í•­]
- êµ¬ë§¤ë‚˜ ì‚¬ìš©ì‹œ ê±±ì •ë˜ëŠ” êµ¬ì²´ì  ë¬¸ì œë“¤
- ì˜ˆì‹œ: "AS í’ˆì§ˆ", "ë‚´êµ¬ì„±", "í˜¸í™˜ì„±", "ì—…ë°ì´íŠ¸ ì§€ì›"

{requirements}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”):
{{
    "ì‚¬ìš©_ì‹œë‚˜ë¦¬ì˜¤": {{
        "ì£¼ìš”_í™œë™": ["ê°„ê²°í•œ í‚¤ì›Œë“œ1", "ê°„ê²°í•œ í‚¤ì›Œë“œ2"],
        "ì‚¬ìš©_í™˜ê²½": ["ê°„ê²°í•œ í‚¤ì›Œë“œ1", "ê°„ê²°í•œ í‚¤ì›Œë“œ2"],
        "ì‚¬ìš©_ì‹œê°„": "ê°„ê²°í•œ í‘œí˜„",
        "ì‚¬ìš©ìž_ìˆ˜ì¤€": "ê°„ê²°í•œ í‘œí˜„"
    }},
    "ì£¼ìš”_ê´€ì‹¬ì‚¬": {{
        "ë¸Œëžœë“œ_ì„ í˜¸ë„": ["ë¸Œëžœë“œ1", "ë¸Œëžœë“œ2"],
        "ë¶ˆíŽ¸ì‚¬í•­": ["ê°„ê²°í•œ í‚¤ì›Œë“œ1", "ê°„ê²°í•œ í‚¤ì›Œë“œ2"],
        "ë§Œì¡±ë„_ì¤‘ìš”í•­ëª©": ["ê°„ê²°í•œ í‚¤ì›Œë“œ1", "ê°„ê²°í•œ í‚¤ì›Œë“œ2"]
    }},
    "ê°ì„±ì _ìš”êµ¬ì‚¬í•­": {{
        "ë””ìžì¸_ì„ í˜¸ë„": ["ê°„ê²°í•œ í‚¤ì›Œë“œ1", "ê°„ê²°í•œ í‚¤ì›Œë“œ2"],
        "ê°€ê²©ëŒ€_ì‹¬ë¦¬": "ê°„ê²°í•œ í‘œí˜„"
    }},
    "ì‚¬ìš©ìž_ìš°ë ¤ì‚¬í•­": ["ê°„ê²°í•œ í‚¤ì›Œë“œ1", "ê°„ê²°í•œ í‚¤ì›Œë“œ2"],
    "ê²€ìƒ‰_í‚¤ì›Œë“œ_ê°€ì¤‘ì¹˜": {{
        "ë†’ì€_ì¤‘ìš”ë„": ["ê°€ìž¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ1", "ê°€ìž¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ2"],
        "ì¤‘ê°„_ì¤‘ìš”ë„": ["ì¤‘ìš”í•œ í‚¤ì›Œë“œ1", "ì¤‘ìš”í•œ í‚¤ì›Œë“œ2"],
        "ë‚®ì€_ì¤‘ìš”ë„": ["ë¶€ê°€ì  í‚¤ì›Œë“œ1", "ë¶€ê°€ì  í‚¤ì›Œë“œ2"]
    }}
}}"""
            
            response = await self.llm.ainvoke(prompt)
            content = response.content if hasattr(response, 'content') else str(response)
            
            # JSON ë¬¸ìžì—´ ì •ì œ
            content = content.strip()
            if content.startswith("```json"):
                content = content.split("```json")[1]
            if content.startswith("```"):
                content = content.split("```")[1]
            content = content.strip()
            
            parsed_content = json.loads(content)
            return {"review_analysis": parsed_content}
        except Exception as e:
            logger.error(f"Error in _prepare_review_agent_state: {e}")
            if content:  # contentê°€ ì¡´ìž¬í•  ë•Œë§Œ ë¡œê·¸
                logger.error(f"Failed content: {content}")
            raise

    async def _prepare_youtube_agent_state(self, requirements: str) -> Dict[str, Any]:
        try:
            prompt = f"""
            ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ ìœ íŠœë¸Œ ë¦¬ë·° ê²€ìƒ‰ì„ ìœ„í•œ ì •ë³´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            
            {requirements}
            
            ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”):
            {{   "query": ["ì‚¬ìš©ìžì˜ ìš”ì²­ì‚¬í•­ì„ ìžì—°ì–´ë¡œ ìƒì„¸ížˆ ê¸°ìˆ "],
                "ê²€ìƒ‰_í‚¤ì›Œë“œ": {{
                    "í•„ìˆ˜_í¬í•¨": ["ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•  í‚¤ì›Œë“œë“¤"],
                    "ì„ íƒ_í¬í•¨": ["í¬í•¨ë˜ë©´ ì¢‹ì„ í‚¤ì›Œë“œë“¤"],
                    "ì œì™¸": ["ì œì™¸í•´ì•¼ í•  í‚¤ì›Œë“œë“¤"]
                }},
                "ë¦¬ë·°_ìœ í˜•": ["ì°¾ì•„ì•¼ í•  ë¦¬ë·° ì˜ìƒ ìœ í˜•ë“¤"],
                "ì¤‘ì _í™•ì¸ì‚¬í•­": ["ì˜ìƒì—ì„œ ì¤‘ì ì ìœ¼ë¡œ í™•ì¸í•´ì•¼ í•  ë‚´ìš©ë“¤"],
                "ìµœì†Œ_ì¡°íšŒìˆ˜": "í•„ìš”í•œ ìµœì†Œ ì¡°íšŒìˆ˜",
                "ì—…ë¡œë“œ_ê¸°ê°„": "ê²€ìƒ‰í•  ê¸°ê°„ ë²”ìœ„"
            }}
            """
            
            response = await self.llm.ainvoke(prompt)
            content = response.content if hasattr(response, 'content') else str(response)
            
            # JSON ë¬¸ìžì—´ ì •ì œ
            content = content.strip()
            if content.startswith("```json"):
                content = content.split("```json")[1]
            if content.startswith("```"):
                content = content.split("```")[1]
            content = content.strip()
            
            parsed_content = json.loads(content)
            return {"youtube_analysis": parsed_content}
        except Exception as e:
            logger.error(f"Error in _prepare_youtube_agent_state: {e}")
            logger.error(f"Failed content: {content}")
            raise
        
    def _combine_collected_info(self, collected_info: Dict[str, str]) -> str:
        base_info = f"""
        ì‚¬ìš© ëª©ì : {collected_info.get('purpose', '')}
        ì„ í˜¸ ë¸Œëžœë“œ: {collected_info.get('preferred_brand', '')}
        ì˜ˆì‚°: {collected_info.get('budget', '')}
        """
        
        # ì¶”ê°€ ìš”êµ¬ì‚¬í•­ì´ ìžˆëŠ” ê²½ìš° í¬í•¨
        if additional_req := collected_info.get('additional_requirements'):
            base_info += f"\nì¶”ê°€ ìš”êµ¬ì‚¬í•­: {additional_req}"
            
        return base_info
        
    async def _proceed_to_requirements(self, collected_info: Dict[str, str], 
                                     conversation_history: List[Dict[str, str]], 
                                     user_input: str) -> Dict[str, Any]:
        combined_input = self._combine_collected_info(collected_info)
        prompt = self.prompts['requirements'].format(user_input=combined_input)
        requirements_response = await self.llm.ainvoke(prompt)
        requirements = requirements_response.content if hasattr(requirements_response, 'content') else str(requirements_response)
        
        return {
            "response": self.prompts['confirmation'].format(requirements=requirements),
            "conversation_history": conversation_history + [
                {"role": "user", "content": user_input},
                {"role": "assistant", "content": self.prompts['confirmation'].format(requirements=requirements)}
            ],
            "requirements": requirements,
            "collected_info": collected_info,
            "status": "confirming_requirements"
        }
        

