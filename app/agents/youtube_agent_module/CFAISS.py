import faiss
import h5py
import numpy as np
import os
import hashlib
from .queue_manager import add_log
import openai
import dotenv
from langchain.schema import Document, BaseRetriever
from pydantic import Field
from typing import Any, List


globalist=[]
def log_wrapper(log_message):
    globalist.append(log_message)
    add_log(log_message)  
class WrIndexFlatL2:
    def __init__(self, dimension,embedingmodel="text-embedding-3-small"):
        dotenv.load_dotenv()
        self.dimension = dimension
        """FAISSì˜ IndexFlatL2ë¥¼ í™•ì¥í•˜ì—¬ metadata ê¸°ëŠ¥ì„ ì¶”ê°€í•œ í´ë˜ìŠ¤"""
        self.index = faiss.IndexFlatL2(dimension)  # FAISS IndexFlatL2 ì´ˆê¸°í™”
        self.metadata = {}  # ë©”íƒ€ë°ì´í„° ì €ì¥ìš© (ì¸ë±ìŠ¤: ë©”íƒ€ë°ì´í„°)
        self.page={} 
        self.text={}
        self.active = []  # ğŸ”¥ ê²€ìƒ‰í•  ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        self.embedingmodel = embedingmodel  # OpenAI ì„ë² ë”© ëª¨ë¸ ì§€ì •
        
    def get_openai_embedding(self, text):
        """OpenAI ìµœì‹  ì„ë² ë”© APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(text, list):
            raise ValueError("ë‹¤ìˆ˜ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ì€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ì¼ ë¬¸ìì—´ë§Œ ì…ë ¥í•˜ì„¸ìš”.")

        response = openai.embeddings.create(
            model=self.embedingmodel,
            input=text
        )
        return np.array(response.data[0].embedding, dtype=np.float32)  # FAISS í˜¸í™˜ float32 ë³€í™˜
    def add(self, datas):
        """ë²¡í„° + ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        if isinstance(datas, dict):
            vectors = datas['vectors']
            metadata_list = datas['metadata'] 
            page_list=datas['page']
            text_list=datas['text']
            assert len(vectors) == len(metadata_list), "ë²¡í„° ê°œìˆ˜ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•¨."
            assert len(vectors) == len(page_list), "ë²¡í„° ê°œìˆ˜ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•¨."
            assert len(vectors) == len(text_list), "ë²¡í„° ê°œìˆ˜ì™€ ì›ë³¸í…ìŠ¤íŠ¸ ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•¨."
            self.index.add(np.array(vectors, dtype=np.float32))  # FAISSì— ì¶”ê°€
            for i, meta in enumerate(metadata_list):
                self.metadata[self.index.ntotal - len(vectors) + i] = meta  # ì¸ë±ìŠ¤ì— ë©”íƒ€ë°ì´í„° ë§¤í•‘
            for i, page in enumerate(page_list):
                self.page[self.index.ntotal - len(vectors) + i] = int(page)  # ì¸ë±ìŠ¤ì— ë©”íƒ€ë°ì´í„° ë§¤í•‘
            for i, text in enumerate(text_list):
                self.text[self.index.ntotal - len(vectors) + i] = text
                    # ğŸ”¥ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì›ë³¸ ë°ì´í„° ì‚­ì œ
            del datas['vectors']
            del datas['metadata']
            del datas['page']    
            del datas['text']
    
    def add_with_embedding(self, datas, model="text-embedding-3-small"):
        if isinstance(datas, dict):
            if isinstance(datas['vectors'], str):
                datas['text'] = [datas['vectors']]
                embeddings = [self.get_openai_embedding(datas['vectors'])]
                datas['vectors'] = embeddings
                self.add(datas)
                del datas
            else:
                raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤.")
def _hash_trans( metadata, page):
    """
    WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, `metadata + page` ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ í•´ì‹œê°’ ìƒì„±
    """

    hash_dict = {}
    for i in range(len(metadata)):
        meta=metadata[i][0]
        unique_key = f"{meta}{page[i][0]}"
        hash_dict[i] = int(hashlib.sha256(unique_key.encode()).hexdigest(), 16) % (2**63)
    return hash_dict  # âœ… í•´ì‹œê°’ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ (index -> hash)


class HDF5VectorDB:
    def __init__(self, filename="vector_db.h5", dimension=128):
        """
        HDF5 ê¸°ë°˜ ë²¡í„° DB
        1. filenameì— ê²½ë¡œë¥¼ ë°›ì•„ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ í´ë” ë° íŒŒì¼ ìƒì„±)
        """
        self.filename = filename
        self.dimension = dimension
        os.makedirs(os.path.dirname(filename), exist_ok=True)  # í´ë” ìƒì„±

        # HDF5 íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(filename):
            with h5py.File(filename, "w") as f:
                f.create_dataset("vectors", shape=(0, dimension), maxshape=(None, dimension), dtype=np.float32)
                f.create_dataset("metadata", shape=(0,), maxshape=(None,), dtype=h5py.string_dtype(encoding='utf-8'))
                f.create_dataset("hash_table", shape=(0,), maxshape=(None,), dtype=np.int64)
                f.create_dataset("page", shape=(0,), maxshape=(None,), dtype=np.int64)
                f.create_dataset("text", shape=(0,), maxshape=(None,), dtype=h5py.string_dtype(encoding='utf-8'))

    def _hash_metadata(self, wr_index):
        """
        WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, `metadata + page` ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ í•´ì‹œê°’ ìƒì„±
        """
        if not isinstance(wr_index, WrIndexFlatL2):
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        hash_dict = {}
        for i, meta in wr_index.metadata.items():
            unique_key = f"{meta}{wr_index.page[i]}"
            hash_dict[i] = int(hashlib.sha256(unique_key.encode()).hexdigest(), 16) % (2**63)

        return hash_dict  # âœ… í•´ì‹œê°’ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ (index -> hash)


    def load_by_indices(self, wr_index):########################################
        """
        WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ë°›ì•„ `metadata + page` ì¡°í•©ì„ í•´ì‹œë¡œ ë³€í™˜ í›„, í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ ë°˜í™˜
        """
        if not isinstance(wr_index, WrIndexFlatL2):
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        # âœ… WrIndexFlatL2 ê°ì²´ì—ì„œ `metadata + page` ì¡°í•©ì„ í•´ì‹œë¡œ ë³€í™˜
        hash_dict = self._hash_metadata(wr_index)
        query_hashes = list(hash_dict.values())  # âœ… í•´ì‹œ ê°’ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        with h5py.File(self.filename, "r") as f:
            hash_table = f["hash_table"][:]
            vectors = f["vectors"][:]
            metadata = f["metadata"][:]
            page = f["page"][:]
            text = f["text"][:]

            matched_indices = np.where(np.isin(hash_table, query_hashes))[0]  # âœ… í•´ë‹¹ í•´ì‹œê°’ì´ ìˆëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°
            if len(matched_indices) == 0:
                return None  # âœ… í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì—†ìŒ

            return {
                "vectors": vectors[matched_indices],
                "metadata": metadata[matched_indices],
                "page": page[matched_indices],
                "text": text[matched_indices]
            }
    def load_by_vactor(self, wr_index):
        """
        WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ë°›ì•„ í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ WrIndexFlatL2 ê°ì²´ë¡œ ë³€í™˜
        """
        if not isinstance(wr_index, WrIndexFlatL2):
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        buf = WrIndexFlatL2(self.dimension)
        add = self.load_by_indices(wr_index)  # âœ… WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬

        if add:
            buf.add(add)  # âœ… WrIndexFlatL2 ê°ì²´ì— ë°ì´í„° ì¶”ê°€
            return buf
        else:
            return None

    def add_vectors(self, wr_index):
        """
        3. WrIndexFlatL2ë¥¼ ì…ë ¥ë°›ì•„ HDF5ì— ì¶”ê°€
        - ì¸ë±ìŠ¤ë¥¼ í•´ì‹œí™”í•´ì„œ ê¸°ì¡´ì´ë‘ ê²¹ì¹˜ë©´ ë®ì–´ì“°ê¸°
        - ë®ì–´ì“°ë©´ log_wrapper ë©”ì‹œì§€ ì¶œë ¥
        """
        if not isinstance(wr_index, WrIndexFlatL2):
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        with h5py.File(self.filename, "a") as f:
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            existing_hash_table = f["hash_table"][:]

            new_vectors = []
            new_metadata = []
            new_hash_table = []
            new_page = []
            new_text = []
            hash_dict = self._hash_metadata(wr_index) 
            for i, meta in wr_index.metadata.items():
                meta_str = str(meta)  # ë¬¸ìì—´ ë³€í™˜
                meta_hash = hash_dict[i] 
                if meta_hash in existing_hash_table:
                    # ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸°
                    index = np.where(existing_hash_table == meta_hash)[0][0]
                    if f["page"][index] == wr_index.page[i]:
                       f["vectors"][index] = wr_index.index.reconstruct(i)
                       f["page"][index] = wr_index.page[i]
                       f["metadata"][index] = meta_str
                       f["text"][index] = wr_index.text[i]
                       log_wrapper(f"[INFO] ê¸°ì¡´ ë°ì´í„° ë®ì–´ì”€: {meta_str}, í˜ì´ì§€: {wr_index.page[i]}")
                else:
                    # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
                    new_vectors.append(wr_index.index.reconstruct(i))
                    new_page.append(wr_index.page[i])
                    new_metadata.append(meta_str)
                    new_text.append(wr_index.text[i])
                    new_hash_table.append(meta_hash)


            # ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¶”ê°€
            if new_vectors:
                n_old = f["vectors"].shape[0]
                n_new = n_old + len(new_vectors)

                # ë²¡í„° ì¶”ê°€
                f["vectors"].resize((n_new, self.dimension))
                f["vectors"][n_old:n_new] = np.array(new_vectors, dtype=np.float32)

                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                f["metadata"].resize((n_new,))
                f["metadata"][n_old:n_new] = new_metadata

                # í•´ì‹œ í…Œì´ë¸” ì¶”ê°€
                f["hash_table"].resize((n_new,))
                f["hash_table"][n_old:n_new] = np.array(new_hash_table, dtype=np.int64)
                
                f["page"].resize((n_new,))
                f["page"][n_old:n_new] = np.array(new_page, dtype=np.int64)
                
                f["text"].resize((n_new,))
                f["text"][n_old:n_new] = new_text

                log_wrapper(f"[INFO] ìƒˆ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ ({len(new_vectors)}ê°œ)")
        # ğŸ”¥ ë©”ëª¨ë¦¬ í•´ì œ: WrIndexFlatL2 ë‚´ë¶€ ë°ì´í„° ì´ˆê¸°í™”
        wr_index.index = faiss.IndexFlatL2(self.dimension)  # FAISS ì¸ë±ìŠ¤ ì¬ì´ˆê¸°í™”
        wr_index.metadata = {}  # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        wr_index.page = {}  # í˜ì´ì§€ ì •ë³´ ì´ˆê¸°í™”
        wr_index.text = {}  # ì›ë³¸ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    
    def extract_custom(self, wr_index):
        """
        WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ë°›ì•„ `metadata + page` ì¡°í•©ì„ í•´ì‹œë¡œ ë³€í™˜ í›„, í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ `self.active`ì— ì €ì¥
        """
        if not isinstance(wr_index, WrIndexFlatL2):
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        # âœ… WrIndexFlatL2 ê°ì²´ì—ì„œ `metadata + page` ì¡°í•©ì„ í•´ì‹œë¡œ ë³€í™˜
        hash_dict = self._hash_metadata(wr_index)
        query_hashes = list(hash_dict.values())  # âœ… í•´ì‹œ ê°’ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        with h5py.File(self.filename, "r") as f:
            hash_table = f["hash_table"][:]

            matched_indices = np.where(np.isin(hash_table, query_hashes))[0]  # âœ… í•´ë‹¹ í•´ì‹œê°’ì´ ìˆëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°

            if len(matched_indices) == 0:
                log_wrapper("<<::STATE::Keyword Search FAIl : To hard filttering>> ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ")
                self.active = []  # ğŸ”¥ ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ activeë¥¼ ë¹„ì›€
            else:
                self.active = matched_indices.tolist()  # ğŸ”¥ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ë¥¼ self.activeì— ì €ì¥
                log_wrapper(f"<<::STATE::Keyword Search SECCEED>> ê²€ìƒ‰ ëŒ€ìƒ ì¸ë±ìŠ¤: {self.active}")
    def extract_custom_from_p_I(self, metadata, page):
        

        # âœ… WrIndexFlatL2 ê°ì²´ì—ì„œ `metadata + page` ì¡°í•©ì„ í•´ì‹œë¡œ ë³€í™˜
        hash_dict = _hash_trans(metadata, page)
        query_hashes = list(hash_dict.values())  # âœ… í•´ì‹œ ê°’ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        with h5py.File(self.filename, "r") as f:
            hash_table = f["hash_table"][:]

            matched_indices = np.where(np.isin(hash_table, query_hashes))[0]  # âœ… í•´ë‹¹ í•´ì‹œê°’ì´ ìˆëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°

            if len(matched_indices) == 0:
                log_wrapper("<<::STATE::Keyword Search FAIl : To hard filttering>> ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ")
                self.active = []  # ğŸ”¥ ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ activeë¥¼ ë¹„ì›€
            else:
                self.active = matched_indices.tolist()  # ğŸ”¥ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ë¥¼ self.activeì— ì €ì¥
                log_wrapper(f"<<::STATE::Keyword Search SECCEED>> ê²€ìƒ‰ ëŒ€ìƒ ì¸ë±ìŠ¤: {self.active}")
        

    def search(self, query_vector, k=5):
        """
        FAISS ê²€ìƒ‰ ìˆ˜í–‰ (self.activeì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì—ì„œë§Œ ê²€ìƒ‰)
        """
        with h5py.File(self.filename, "r") as f:
            vectors = f["vectors"][:]
            metadata = f["metadata"][:]
            pages = f["page"][:]
            texts = f["text"][:]

        if len(vectors) == 0 or len(self.active) == 0:
            return None, None, None  # ğŸ”¥ ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜

        # ğŸ”¥ self.activeì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë§Œ FAISSì— ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰
        active_vectors = np.array([vectors[i] for i in self.active], dtype=np.float32)
        active_metadata = [metadata[i] for i in self.active]
        active_pages = [pages[i] for i in self.active]
        active_texts = [texts[i] for i in self.active]

        # ğŸ”¥ FAISS ì°¨ì›ê³¼ ë²¡í„° ì°¨ì›ì´ ë§ëŠ”ì§€ í™•ì¸ (`self.index.d` ëŒ€ì‹  `self.dimension` ì‚¬ìš©)
        if active_vectors.shape[1] != self.dimension:
            log_wrapper(f"<<::STATE::Critical raise ValueError >>ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜! FAISS ì¸ë±ìŠ¤ ì°¨ì›: {self.dimension}, ì…ë ¥ ë²¡í„° ì°¨ì›: {active_vectors.shape[1]}")
            raise ValueError(f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜! FAISS ì¸ë±ìŠ¤ ì°¨ì›: {self.dimension}, ì…ë ¥ ë²¡í„° ì°¨ì›: {active_vectors.shape[1]}")

        # ğŸ”¥ ë©”ëª¨ë¦¬ ì •ë ¬ ìµœì í™” (FAISSê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        active_vectors = np.ascontiguousarray(active_vectors, dtype=np.float32)

        index = faiss.IndexFlatL2(self.dimension)  # ğŸ”¥ ê²€ìƒ‰ì„ ìœ„í•œ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±
        index.add(active_vectors)  # ğŸ”¥ self.activeì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë§Œ ê²€ìƒ‰ ì¸ë±ìŠ¤ì— ì¶”ê°€

        query_vector = np.array(query_vector, dtype=np.float32).reshape(1, -1)  # ğŸ”¥ FAISSê°€ ìš”êµ¬í•˜ëŠ” 2D í˜•íƒœë¡œ ë³€í™˜
        distances, indices = index.search(query_vector, k)  # ğŸ”¥ FAISS ê²€ìƒ‰ ìˆ˜í–‰

        # ğŸ”¥ WrIndexFlatL2 ê°ì²´ ìƒì„± ë° ê²€ìƒ‰ëœ ë°ì´í„° ì¶”ê°€
        search_result = WrIndexFlatL2(self.dimension)
        search_data = {"vectors": [], "metadata": [], "page": [], "text": []}

        for idx in indices[0]:
            if idx != -1:
                search_data["vectors"].append(active_vectors[idx])  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                search_data["metadata"].append(active_metadata[idx].decode('utf8'))  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                search_data["page"].append(active_pages[idx])  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                search_data["text"].append(active_texts[idx].decode('utf8'))  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

        search_result.add(search_data)        

        return self.to_document(search_result), distances, indices


    def to_document(self, data):
        """
        WrIndexFlatL2 ê°ì²´ë¥¼ ì…ë ¥ë°›ì•„, `self.active` ë‚´ë¶€ì˜ ë°ì´í„°ë§Œ ë³€í™˜í•˜ì—¬ LangChain Document ê°ì²´ë¡œ ë³€í™˜
        """
        if not isinstance(data, WrIndexFlatL2):
            log_wrapper("<<::STATE::Critical raise ValueError >>ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” WrIndexFlatL2 ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        if len(self.active) == 0:
            log_wrapper("[INFO] ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        hesh=self._hash_metadata(data)
        docsout = []

        with h5py.File(self.filename, "r+") as f:
            vectors = f["vectors"][:]
            metadata = f["metadata"][:]
            pages = f["page"][:]
            texts = f["text"][:]
            hash_table = f["hash_table"][:]
            hash_to_idx = {hash_table[idx]: idx for idx in self.active}
            # âœ… `self.active` ë‚´ë¶€ì˜ ë°ì´í„°ë§Œ ë³€í™˜
            total_hash=[]
            for values in hash_to_idx.keys():
                total_hash.append(values)
            for meta_hesh in hesh.values():
                meta_hesh=np.int64(meta_hesh)
                if meta_hesh is None or meta_hesh not in total_hash:
                    log_wrapper(f"[WARNING] `Hesh : {meta_hesh}`ëŠ” WrIndexFlatL2ì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ. ê±´ë„ˆëœ€.")
                    continue

                # âœ… ë³€í™˜ëœ ë°ì´í„° ì¶”ê°€
                page_content = texts[hash_to_idx[meta_hesh]]  # âœ… ì›ë³¸ í…ìŠ¤íŠ¸ í™œìš©
                metadata_dict = {
                    "index": metadata[hash_to_idx[meta_hesh]].decode("utf-8"),
                    "page": pages[hash_to_idx[meta_hesh]],
                    "vectors": vectors[hash_to_idx[meta_hesh]]  # âœ… HDF5ì—ì„œ ì§ì ‘ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
                }
                docsout.append(Document(page_content=page_content, metadata=metadata_dict))

        return docsout  # âœ… ê²€ìƒ‰ëœ ë°ì´í„°ë§Œ ë³€í™˜í•˜ì—¬ ë°˜í™˜



    def from_document(self, doc):
        """
        LangChain Document ê°ì²´ë¥¼ ë‹¤ì‹œ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” WrIndexFlatL2 í¬ë§·ìœ¼ë¡œ ë³€í™˜
        - ë²¡í„°ë¥¼ ë‹¤ì‹œ ì„ë² ë”©í•˜ì§€ ì•Šê³ , metadata["vectors"]ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ WrIndexFlatL2ì— ì¶”ê°€
        """
        if isinstance(doc, list):
            out = WrIndexFlatL2(self.dimension)
            for d in doc:
                if isinstance(d, Document):
                    page_content = d.page_content
                    metadata = d.metadata
                    if "vectors" not in metadata:
                        log_wrapper("<<::STATE::Critical raise ValueError >>âŒ ë²¡í„° ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ Document ê°ì²´ì…ë‹ˆë‹¤.")
                        raise ValueError("âŒ ë²¡í„° ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ Document ê°ì²´ì…ë‹ˆë‹¤.")
                    out.add({
                        "vectors": [metadata["vectors"]],  # âœ… ê¸°ì¡´ ë²¡í„°ë¥¼ ê·¸ëŒ€ë¡œ WrIndexFlatL2ì— ì¶”ê°€
                        "metadata": [metadata.get("index", None)],
                        "page": [metadata.get("page", None)],
                        "text": [page_content]
                    })
            return out

        elif isinstance(doc, Document):
            page_content = doc.page_content
            metadata = doc.metadata
            if "vectors" not in metadata:
                raise ValueError("âŒ ë²¡í„° ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ Document ê°ì²´ì…ë‹ˆë‹¤.")
            out = WrIndexFlatL2(self.dimension)
            out.add({
                "vectors": [metadata["vectors"]],  # âœ… ê¸°ì¡´ ë²¡í„°ë¥¼ ê·¸ëŒ€ë¡œ WrIndexFlatL2ì— ì¶”ê°€
                "metadata": [metadata.get("index", None)],
                "page": [metadata.get("page", None)],
                "text": [page_content]
            })
            return out

        else:
            raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” Document ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    class RetrieverAdapter(BaseRetriever):
        mon: Any = Field(...)  # HDF5VectorDB ì¸ìŠ¤í„´ìŠ¤ (í•„ìˆ˜ í•„ë“œ)
        k: int = Field(default=5)
        """
        HDF5VectorDB ë‚´ë¶€ì— ì¡´ì¬í•˜ëŠ” RetrieverAdapter í´ë˜ìŠ¤.
        LangChain retriever ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ HDF5VectorDBì˜ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•¨.
        """


        def get_relevant_documents(self, query: str, **kwargs) -> List[Document]:
            """
            ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê´€ë ¨ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
            """ 
            tools=WrIndexFlatL2(self.mon.dimension)
            out=tools.get_openai_embedding(query)
            if not self.mon.active:
                raise ValueError("ê²€ìƒ‰í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¡°ê±´ì„ êµ¬ì²´í™” í•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            else:
                out,_,_=self.mon.search(out, self.k)
                return out
        @property
        def search_kwargs(self) -> dict:
            return {}
        class Config:
            arbitrary_types_allowed = True
    
    
    def as_retriever(self, k: int = 5):
        """
        HDF5VectorDBì˜ retriever ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        LangChain ì²´ì¸ì—ì„œ ì´ ê°ì²´ì˜ get_relevant_documents(query: str) ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        return HDF5VectorDB.RetrieverAdapter(mon=self, k=k)



if __name__ == "__main__":

    log_wrapper("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n")
    
