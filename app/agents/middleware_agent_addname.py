import asyncio
import os
import json
import logging
from typing import Dict, Any
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

logger = logging.getLogger("smartpick.agents.middleware_agent")
load_dotenv()

class MiddlewareAgent:
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3, api_key=self.openai_api_key)

    async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        MiddlewareAgentëŠ” parallel_analysisì—ì„œ ì „ë‹¬ë°›ì€ stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„
        LLMì„ í™œìš©í•˜ì—¬ ìµœì¢… ì œí’ˆ ì¶”ì²œì„ ìƒì„±í•œë‹¤.
        """
        logger.debug(f"MiddlewareAgent ì‹¤í–‰: {state}")
        print("ğŸ” MiddlewareAgent ì‹œì‘...")

        # 1ï¸âƒ£ ë³‘ë ¬ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (None ë°©ì§€)
        review_results = state.get("review_results") or {}  # Fallback to empty dict
        spec_results = state.get("spec_results") or {}
        youtube_results = state.get("youtube_results") or {}

        # 2ï¸âƒ£ LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì œí’ˆ ì¶”ì²œ ìƒì„±
        final_recommendation = await self.generate_final_recommendation(review_results, spec_results, youtube_results)

        return {"middleware_results": final_recommendation} if final_recommendation else {"error": "ìµœì¢… ì¶”ì²œ ì‹¤íŒ¨"}

    async def generate_final_recommendation(self, review_data, spec_data, youtube_data):
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì¶”ì²œ ì œí’ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜.
        """
        print("ğŸ§  LLMì„ í™œìš©í•œ ìµœì¢… ì œí’ˆ ì¶”ì²œ ìƒì„±...")

        try:
            # 3ï¸âƒ£ ì•ˆì „í•œ ë°ì´í„° ì‚¬ìš© (ê°’ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©)
            llm_input = {
                "ì‚¬ìš©ì ë¦¬ë·° ë¶„ì„": review_data.get("recommendations", ["ë¦¬ë·° ë°ì´í„° ì—†ìŒ"]),
                "ì œí’ˆ ìŠ¤í™ ì¶”ì²œ": spec_data.get("ì¶”ì²œ ì œí’ˆ", ["ìŠ¤í™ ë°ì´í„° ì—†ìŒ"]),
                "ìœ íŠœë¸Œ ë¦¬ë·° ë¶„ì„": youtube_data.get("reviews", ["ìœ íŠœë¸Œ ë¦¬ë·° ë°ì´í„° ì—†ìŒ"])
            }

            # 4ï¸âƒ£ LLM í˜¸ì¶œ (JSON ì¶œë ¥ì„ ê°•ì œ)
            response = await self.llm.ainvoke([
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ìµœê³ ì˜ ì œí’ˆ ì¶”ì²œ AIì…ë‹ˆë‹¤. "
                            "ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­, ì œí’ˆ ìŠ¤í™, ì‚¬ìš©ì ë¦¬ë·°, ìœ íŠœë¸Œ ë¦¬ë·°ë¥¼ ì¢…í•©í•˜ì—¬ "
                            "ìµœì ì˜ ì œí’ˆëª…ì„ **ë°˜ë“œì‹œ JSON í˜•ì‹**ìœ¼ë¡œ 3ê°œë§Œ ì¶œë ¥í•˜ì„¸ìš”. "
                            "ì„¤ëª… ì—†ì´ JSON ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”. "
                            "ì˜ˆì œ ì¶œë ¥: { \"ìµœì¢… ì¶”ì²œ ì œí’ˆ\": [\"ì œí’ˆ1\", \"ì œí’ˆ2\", \"ì œí’ˆ3\"] }"
                },
                {
                    "role": "user",
                    "content": json.dumps(llm_input, ensure_ascii=False)
                }
            ])

            # 5ï¸âƒ£ ì‘ë‹µ ì²˜ë¦¬
            response_text = response.content.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:-3].strip()  # ì½”ë“œ ë¸”ë¡ ì œê±°

            print("ğŸ¯ LLM ìµœì¢… ì¶”ì²œ:", response_text)

            # JSON ë³€í™˜ ì‹œë„
            final_output = json.loads(response_text)

            # ğŸ”¥ 6ï¸âƒ£ ê²°ê³¼ê°€ 3ê°œê°€ ì•„ë‹Œ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            if "ìµœì¢… ì¶”ì²œ ì œí’ˆ" in final_output and isinstance(final_output["ìµœì¢… ì¶”ì²œ ì œí’ˆ"], list):
                final_output["ìµœì¢… ì¶”ì²œ ì œí’ˆ"] = final_output["ìµœì¢… ì¶”ì²œ ì œí’ˆ"][:3]  # 3ê°œë¡œ ì œí•œ
            else:
                raise ValueError("LLMì´ ì˜ëª»ëœ í˜•ì‹ì˜ ì¶œë ¥ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

            return final_output

        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"JSON ë³€í™˜ ì‹¤íŒ¨: {e}, ì‘ë‹µ ë‚´ìš©: {response_text}")
            return {"error": "ìµœì¢… ì¶”ì²œì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
